# The Google File System
# 摘要：
&ensp;&ensp;我们设计并实施了谷歌文件系统，一个可扩展的分布式的文件系统用于大型分布式数据密集型的应用程序，它在廉价的硬件上运行提供了容错。并且它为大量的客户端提供高性能。  
&ensp;&ensp;虽然与以前的分布式系统有很多目标重叠，我们的设计是由我们的应用程序工作负载和技术环境驱动的，这反映了一些早期文件系统设想的明显不同，导致了我们重新审视传统选择并探索截然不同的设计点。  
&ensp;&ensp;这个文件系统成功的满足了我们的存储需要，它作为我们的服务所使用的数据生成和处理以及需要大型数据库集合的研发工作的存储平台被广泛部署在谷歌内部。迄今为止最大的集群在超过1000台机器上的数千个磁盘上提供了数百TB的存储空间，并且由数百个客户端同时访问。  
&ensp;&ensp;在本文中，我们介绍了旨在支持分布式应用程序的文件系统接口的扩展，讨论了我们设计的许多方面，并报告了来自微基准和现实世界使用的测量结果。
# 1. 介绍：
&ensp;&ensp;我们设计并实施了GFS以满足google数据处理快速增长的需求，GFS与以前的分布式文件系统有着许多相同的目标，例如性能、可扩展性、可靠性、可用性。但是它的设计是由我们的应用程序工作负载和技术环境驱动的，这些观察结果反映了与一些早期文件系统的设计和假设有着显著的偏离，我们重新审视了传统的选择，探索了设计空间中截然不同的观点。  
&ensp;&ensp;首先，组件故障是常态而不是例外，文件系统考虑几百甚至是几千台由廉价的上也不见构建的存储机器组成，并由相当多数量的客户端访问，组件的数量和质量实际上保证了某些组件在任何给定的时间都无法正常工作，而有些组件无法从当前的故障中恢复。我们已经看到由应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障引起的问题。因此，持续的监控、错误检测、容错和自动恢复必须是系统的组成部分。  
&ensp;&ensp;其次，按照传统的标准，文件非常大，若干GB的文件很普遍，每个文件通常包含许多应用对象，比如网络文件。当我们经常处理由数十亿个对象组成的TB级且快速增长的数据集时，即使文件系统可以支持，也很难管理数十亿个大约KB大小的文件。因此必须重新审视设计设想和参数，如I/O操作和块大小。  
&ensp;&ensp;第三，大多数文件都是通过附加新数据而不是覆盖现有数据进行修改的，实际上文件中几乎不存在随机写入。一旦写入，文件就只能读取，而且通常只能顺序读取。各种数据都具有这些特性。有些可能构成数据分析程序扫描的大型数据库。有些可能是通过运行应用程序连续生成的数据流。有些可能是档案数据。有些可能是在一台机器上产生并在另一台机器上处理的中间结果，无论是同时还是稍后。考虑到这种对大型文件的访问模式，追加成为性能优化和原子性保证的焦点，而在客户端缓存数据块则失去了吸引力。  
&ensp;&ensp;第四。通过增加灵活性，共同设计应用程序和文件系统API有利于整个系统。例如我们放宽了GFS一致性模型，大大简化了文件系统，而不会给应用程序带来沉重的负担，我们还引入了一个原子追加操作，这样多个客户机可以同时追加到一个文件，而无需在他们之间进行额外的同步，这些将在后面进行更详细的讨论。  
&ensp;&ensp;目前为不同的目的部署了多个GFS集群，最大的有超过1000个存储节点，超过三百tb数据，并且被上百个客户端持续高频访问。  
# 2. 设计概览：
## 2.1 设想
&ensp;&ensp;在为我们的需求设计文件系统时，我们一直以提供挑战和机遇的假设为指导，我们之前提到了一些关键的观察结果，现在更详细的阐述我们的假设  
该系统由许多经常会发生故障的廉价商业组件构成。它必须持续监控自身，并定期检测、容忍组件故障，并及时从中恢复。  
* 该系统存储少了大文件，我们预计会有几百万个小文件，每个文件通常为100MB或者更大，若干GB是常见的情况，应该有效的进行管理，必须支持小文件，但我们不需要对他们进行优化。
* 工作负载主要包括两种读取，大型流式读取和小型随机读取。在大型流式读取中单个操作通常读取数百KB，更常见的是1MB或者更多，来自同一个客户端的联系操作通常读取文件的连续区域，一个小的随机读取通常以任意偏移量读取几KB，注重性能的应用程序通常对他们的小读取进行批处理和排序，以便在文件中稳定的前进，而不是来回跳跃移动。
* 工作负载也有许多文件追加的大型顺序写入，典型的操作大小与读取操作大小相似。一旦写入，文件很少在被修改。支持文件中任意位置的小写入，但不一定要高效。
* 系统必须为同时附加再同一个文件的许多客户端有效的实现定义良好的语义，我们的文件经常作为生产者消费者队列或多路合并。数百个生产者，每台机器运行一个，将同时追加到同一个文件中，具有最小同步开销的原子性是必不可少的。该文件很可能稍后会被读取，或者消费者可能正在同时读取该文件。
* 高持续的带宽比低延迟更重要，我们的大多数目标都是非常重视以高速率批量处理数据，而很少有人对单个读取或写入有严格的响应时间要求。

## 2.2 接口
&ensp;&ensp;GFS 提供一个熟悉的文件系统接口，尽管它没有实现像POSIX之类的标准API。文件在目录中分层组织并由路径名标识，我们支持创建、删除、打开、关闭、读取和写入这些文件的常规操作。  
&ensp;&ensp;而且 GFS 有快照和记录追加的操作，快照以低成本创建文件或目录树的副本。记录追加允许多个客户端同时向一个文件追加数据，同时保证每个客户端追加的原子性。它对于实现多路合并结果和生产者消费者队列很有用，许多客户端可以同时附加到这些队列，而无需额外的锁定，我们发现这些类型的文件对于构建大型分布式应用程序非常宝贵，快照和记录追加分别在第3.4节和3.3节中进一步讨论。  
## 2.3 架构
&ensp;&ensp;GFS集群是由一个master和多个chunkserver组成，被多个client访问。就像图1展现的那样，每一个通常都是允许用户级服务器进程的商品级linux机器。在同一台机器上同时运行chunkserver和client是很容易的，只要机器的资源允许并且能接受因为所运行的应用程序可能是不稳定而导致较低的可靠性。  
&ensp;&ensp;文件被分成固定大小的块，这些块由主节点在创建块时分配的固定且全局唯一的64位块柄标识。块服务器将块作为Linux文件存储在本地磁盘，并读取或写入由块句柄和字节范围指定的块数据。为了可靠性，每一个块都会在多个块服务器上复制。默认情况下，我们存储三个副本，尽管用户可以为文件命名空间的不同区域指定不同的复制级别。  
&ensp;&ensp;主服务器维护所有文件系统元数据。这包括命名空间、访问控制信息、从文件到块的映射以及块的当前位置。它还控制系统范围的活动，例如块租用管理、孤立块的垃圾回收以及块服务器之间的块迁移。主服务器通过HeartBeat消息与每个块服务器进行周期性的通信，为其提供指令并收集其状态。  
&ensp;&ensp;链接到每个应用程序的GFS客户端代码实现文件系统API，并与主服务器和块服务器通信以代表应用程序读取或写入数据。客户端与主服务器交互以进行元数据操作，但所有承载数据的通信都直接进入块服务器。我们不提供POSIX API，因此不需要挂钩到Linux vnode层  
&ensp;&ensp;客户端和服务器都不会缓存文件数据。客户端缓存几乎没有什么好处，因为大多数应用程序都是通过大型文件流传输的，或者工作集太大而无法缓存。没有它们可以通过消除缓存一致性问题来简化客户端和整个系统。（但是客户端会缓存元数据）块服务器不需要文件数据，因为块被存储为本地文件，因此Linux的缓冲区缓存已经将经常访问的数据保存在内存中。  
## 2.4 单个主节点
	